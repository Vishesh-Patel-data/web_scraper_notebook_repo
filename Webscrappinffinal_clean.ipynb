{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cac282-6a8a-49f2-af48-e789dc03b083",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# the below script can give the dataset for the housing data set with individual link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea6864-6c96-4543-908e-fea925101000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Found 15 tables.\n",
    "\n",
    "Table 1:\n",
    "               0   1\n",
    "0  BRIDGEPORT,CT NaN\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 2:\n",
    "   Valuation Year Improvements      Land     Total\n",
    "0            2024     $108,720  $112,720  $221,440\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 3:\n",
    "   Valuation Year Improvements     Land     Total\n",
    "0            2024      $76,100  $78,900  $155,000\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 4:\n",
    "          0                                       1\n",
    "0     Owner                 LARES JOSE J & PATRICIA\n",
    "1  Co-Owner                                     NaN\n",
    "2   Address  48 GASPEE RD BRIDGEPORT, CT 06606-1709\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 5:\n",
    "             0          1\n",
    "0   Sale Price   $250,000\n",
    "1  Certificate        NaN\n",
    "2  Book & Page  10284/207\n",
    "3          NaN        NaN\n",
    "4          NaN        NaN\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 6:\n",
    "                            Owner Sale Price  Certificate Book & Page  \\\n",
    "0         LARES JOSE J & PATRICIA   $250,000          NaN   10284/207   \n",
    "1                  ESPINOSA LUISA         $0          NaN   10036/127   \n",
    "2  ESPINOSA LUIS & LUISA ESPINOSA   $258,000          NaN   6304/0100   \n",
    "3              ROSENKRANZ SARAH L   $175,000          NaN   4824/0245   \n",
    "4    SAMPAIO AMILCAR & LEOPOLDINA   $132,000          NaN   4232/0069   \n",
    "\n",
    "  Instrument   Sale Date  \n",
    "0         00  09/21/2020  \n",
    "1         01  06/10/2019  \n",
    "2       UNKQ  03/21/2005  \n",
    "3       UNKQ  02/19/2002  \n",
    "4       UNKQ  11/01/1999  \n",
    "\n",
    "========================================\n",
    "\n",
    "Table 7:\n",
    "                                      0         1\n",
    "0                           Year Built:      1960\n",
    "1                          Living Area:      1092\n",
    "2                     Replacement Cost:  $155,318\n",
    "3                Building Percent Good:        70\n",
    "4  Replacement Cost  Less Depreciation:  $108,720\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 8:\n",
    "        Field  Description\n",
    "0      Style:        Ranch\n",
    "1       Model  Residential\n",
    "2      Grade:            C\n",
    "3    Stories:         1.00\n",
    "4  Occupancy:            1\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 9:\n",
    "  Code                 Description  Gross Area  Living Area\n",
    "0  BAS                 First Floor        1092         1092\n",
    "1  BSM                    Basement        1092            0\n",
    "2  FEP              Enclosed Porch         212            0\n",
    "3  PTO                       Patio         145            0\n",
    "4  UST  Unfinished Utility Storage         286            0\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 10:\n",
    "                            0                           1  \\\n",
    "0  No Data for Extra Features  No Data for Extra Features   \n",
    "\n",
    "                            2                           3  \\\n",
    "0  No Data for Extra Features  No Data for Extra Features   \n",
    "\n",
    "                            4                           5  \\\n",
    "0  No Data for Extra Features  No Data for Extra Features   \n",
    "\n",
    "                            6                           7  \\\n",
    "0  No Data for Extra Features  No Data for Extra Features   \n",
    "\n",
    "                            8  \n",
    "0  No Data for Extra Features  \n",
    "\n",
    "========================================\n",
    "\n",
    "Table 11:\n",
    "               0              1\n",
    "0       Use Code            101\n",
    "1    Description  Single Family\n",
    "2           Zone             RA\n",
    "3   Neighborhood             20\n",
    "4  Alt Land Appr             No\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 12:\n",
    "                 0         1\n",
    "0     Size (Acres)      0.20\n",
    "1         Frontage         0\n",
    "2            Depth         0\n",
    "3   Assessed Value   $78,900\n",
    "4  Appraised Value  $112,720\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 13:\n",
    "                          0                         1  \\\n",
    "0  No Data for Outbuildings  No Data for Outbuildings   \n",
    "\n",
    "                          2                         3  \\\n",
    "0  No Data for Outbuildings  No Data for Outbuildings   \n",
    "\n",
    "                          4                         5  \\\n",
    "0  No Data for Outbuildings  No Data for Outbuildings   \n",
    "\n",
    "                          6                         7  \\\n",
    "0  No Data for Outbuildings  No Data for Outbuildings   \n",
    "\n",
    "                          8  \n",
    "0  No Data for Outbuildings  \n",
    "\n",
    "========================================\n",
    "\n",
    "Table 14:\n",
    "   Valuation Year Improvements      Land     Total\n",
    "0            2023     $108,720  $112,720  $221,440\n",
    "1            2022     $108,720  $112,720  $221,440\n",
    "2            2021     $108,720  $112,720  $221,440\n",
    "\n",
    "========================================\n",
    "\n",
    "Table 15:\n",
    "   Valuation Year Improvements     Land     Total\n",
    "0            2023      $76,100  $78,900  $155,000\n",
    "1            2022      $76,100  $78,900  $155,000\n",
    "2            2021      $76,100  $78,900  $155,000\n",
    "\n",
    "========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91038ffd-fc92-496b-bde4-5c110b6f7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=1763\",\n",
    "    \"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=2326\",\n",
    "    \"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=1764\",\n",
    "    \"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=27051\",\n",
    "\"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=26995\",\n",
    "\"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=27052\",\n",
    "]\n",
    "\n",
    "# Function to fetch page and parse the HTML\n",
    "def fetch_page(url):\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the entire page content\n",
    "    page_content = soup.prettify()\n",
    "    \n",
    "    return page_content\n",
    "\n",
    "# Function to process and extract data from the page\n",
    "def process_page(page_content):\n",
    "    # Use pandas to read the tables from the HTML\n",
    "    tables = pd.read_html(page_content)\n",
    "\n",
    "    # Extract specific tables (Table 6, Table 9, and Table 4)\n",
    "    table_6 = tables[5]  # Table 6 is at index 5\n",
    "    table_9 = tables[8]  # Table 9 is at index 8\n",
    "    table_4 = tables[3]  # Table 4 is at index 3\n",
    "\n",
    "    # Extract \"Owner\" from the first row, second column in Table 4\n",
    "    owner = table_4.iloc[0, 1]\n",
    "\n",
    "    # Extract \"Address\" from the third row, second column in Table 4\n",
    "    address = table_4.iloc[2, 1]\n",
    "\n",
    "    # Extract the pincode from the address (assuming the pincode is the last part of the address)\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "\n",
    "    # Filter out \"Co-Owner\" and \"Address\" rows, keeping only \"Owner\"\n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "\n",
    "    # Add address and pincode columns to each table\n",
    "    table_6['Address'] = address\n",
    "    table_6['Pincode'] = pincode\n",
    "\n",
    "    table_9['Address'] = address\n",
    "    table_9['Pincode'] = pincode\n",
    "\n",
    "    # Using .loc to modify the cleaned table, avoiding the SettingWithCopyWarning\n",
    "    table_4_cleaned.loc[:, 'Address'] = address\n",
    "    table_4_cleaned.loc[:, 'Pincode'] = pincode\n",
    "\n",
    "    return table_6, table_9, table_4_cleaned\n",
    "\n",
    "# Lists to store the combined tables for all URLs\n",
    "combined_table_6 = []\n",
    "combined_table_9 = []\n",
    "combined_table_4_cleaned = []\n",
    "\n",
    "# Scrape and process each URL\n",
    "for url in urls:\n",
    "    print(f\"Scraping {url}...\")\n",
    "    page_content = fetch_page(url)\n",
    "    table_6, table_9, table_4_cleaned = process_page(page_content)\n",
    "    \n",
    "    # Append the processed tables to the combined lists\n",
    "    combined_table_6.append(table_6)\n",
    "    combined_table_9.append(table_9)\n",
    "    combined_table_4_cleaned.append(table_4_cleaned)\n",
    "\n",
    "# Concatenate the data from all URLs for each table\n",
    "final_table_6 = pd.concat(combined_table_6, ignore_index=True)\n",
    "final_table_9 = pd.concat(combined_table_9, ignore_index=True)\n",
    "final_table_4_cleaned = pd.concat(combined_table_4_cleaned, ignore_index=True)\n",
    "\n",
    "# Create a new Excel file with 3 sheets\n",
    "with pd.ExcelWriter('property_details_combined.xlsx') as writer:\n",
    "    final_table_6.to_excel(writer, sheet_name='Sales Data', index=False)\n",
    "    final_table_9.to_excel(writer, sheet_name='Area Details', index=False)\n",
    "    final_table_4_cleaned.to_excel(writer, sheet_name='Owner Details', index=False)\n",
    "\n",
    "print(\"All tables have been combined and saved to property_details_combined.xlsx.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2211ba-5243-4511-910e-6d0b6a0f639b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pandas beautifulsoup4 lxml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89889db-bde3-4d9a-974f-05644d5f6df7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# the below script can give the datset for all the houses present on any street or letter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa2a1f-6f6e-4382-a4c3-84979e6d94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working on this to scrap the whole database of bridgeport\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Function to fetch the main street page and extract all the street URLs\n",
    "def fetch_main_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    street_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Streets.aspx?Name=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return street_links\n",
    "\n",
    "# Function to fetch the street page and extract all property links\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "# Function to fetch page and parse the HTML\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "# Function to process and extract data from the page\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "    \n",
    "    table_6 = tables[5]\n",
    "    table_9 = tables[8]\n",
    "    table_4 = tables[3]\n",
    "    table_2 = tables[1]\n",
    "    table_3 = tables[2]\n",
    "    table_14 = tables[13]\n",
    "    table_15 = tables[14]\n",
    "    table_7 = tables[6]\n",
    "    table_11 = tables[10]\n",
    "    table_12 = tables[11]\n",
    "    \n",
    "    owner = table_4.iloc[0, 1]\n",
    "    address = table_4.iloc[2, 1]\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    \n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15]:\n",
    "        table['Address'] = address\n",
    "        table['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 7 to a horizontal format\n",
    "    table_7_transposed = table_7.set_index(table_7.columns[0]).T.reset_index(drop=True)\n",
    "    table_7_transposed['Address'] = address\n",
    "    table_7_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 11 to a horizontal format\n",
    "    table_11_transposed = table_11.set_index(table_11.columns[0]).T.reset_index(drop=True)\n",
    "    table_11_transposed['Address'] = address\n",
    "    table_11_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 12 to a horizontal format\n",
    "    table_12_transposed = table_12.set_index(table_12.columns[0]).T.reset_index(drop=True)\n",
    "    table_12_transposed['Address'] = address\n",
    "    table_12_transposed['Pincode'] = pincode\n",
    "    \n",
    "    return table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_11_transposed, table_12_transposed\n",
    "\n",
    "# Initialize lists to store all scraped data\n",
    "combined_table_6 = []\n",
    "combined_table_9 = []\n",
    "combined_table_4_cleaned = []\n",
    "combined_table_2 = []\n",
    "combined_table_3 = []\n",
    "combined_table_14 = []\n",
    "combined_table_15 = []\n",
    "combined_table_7 = []\n",
    "combined_table_11 = []\n",
    "combined_table_12 = []\n",
    "\n",
    "# Main URL for scraping all street links\n",
    "main_url = \"https://gis.vgsi.com/bridgeportct/Streets.aspx\"\n",
    "street_links = fetch_main_page(main_url)\n",
    "\n",
    "# Iterate through each street URL and scrape data\n",
    "for street_url in street_links:\n",
    "    print(f\"Processing {street_url}\")\n",
    "    \n",
    "    # Fetch all property links from the street page\n",
    "    property_links = fetch_street_page(street_url)\n",
    "    \n",
    "    # Scrape and process each property URL\n",
    "    for url in property_links:\n",
    "        page_content = fetch_page(url)\n",
    "        table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_11_transposed, table_12_transposed = process_page(page_content)\n",
    "        \n",
    "        combined_table_6.append(table_6)\n",
    "        combined_table_9.append(table_9)\n",
    "        combined_table_4_cleaned.append(table_4_cleaned)\n",
    "        combined_table_2.append(table_2)\n",
    "        combined_table_3.append(table_3)\n",
    "        combined_table_14.append(table_14)\n",
    "        combined_table_15.append(table_15)\n",
    "        combined_table_7.append(table_7_transposed)\n",
    "        combined_table_11.append(table_11_transposed)\n",
    "        combined_table_12.append(table_12_transposed)\n",
    "\n",
    "# Concatenate all the data into final tables\n",
    "final_table_6 = pd.concat(combined_table_6, ignore_index=True)\n",
    "final_table_9 = pd.concat(combined_table_9, ignore_index=True)\n",
    "final_table_4_cleaned = pd.concat(combined_table_4_cleaned, ignore_index=True)\n",
    "final_table_2 = pd.concat(combined_table_2, ignore_index=True)\n",
    "final_table_3 = pd.concat(combined_table_3, ignore_index=True)\n",
    "final_table_14 = pd.concat(combined_table_14, ignore_index=True)\n",
    "final_table_15 = pd.concat(combined_table_15, ignore_index=True)\n",
    "final_table_7 = pd.concat(combined_table_7, ignore_index=True)\n",
    "final_table_11 = pd.concat(combined_table_11, ignore_index=True)\n",
    "final_table_12 = pd.concat(combined_table_12, ignore_index=True)\n",
    "\n",
    "# Create a new Excel file with 11 sheets (removed Table 8)\n",
    "with pd.ExcelWriter('property_details_combined_all_streets_full_website.xlsx') as writer:\n",
    "    final_table_6.to_excel(writer, sheet_name='Sales Data', index=False)\n",
    "    final_table_9.to_excel(writer, sheet_name='Area Details', index=False)\n",
    "    final_table_4_cleaned.to_excel(writer, sheet_name='Owner Details', index=False)\n",
    "    final_table_2.to_excel(writer, sheet_name='Appraisal', index=False)\n",
    "    final_table_3.to_excel(writer, sheet_name='Assessment', index=False)\n",
    "    final_table_14.to_excel(writer, sheet_name='Past Appraisal', index=False)\n",
    "    final_table_15.to_excel(writer, sheet_name='Past Assessment', index=False)\n",
    "    final_table_7.to_excel(writer, sheet_name='Building Details', index=False)\n",
    "    final_table_11.to_excel(writer, sheet_name='Use and Zone', index=False)\n",
    "    final_table_12.to_excel(writer, sheet_name='Size and Value', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955c256-aaf4-4141-bd8f-f82eb3aea121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this scrip will scrap whole I street ,  table 8 has removed becuz of concate issues\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL of the main page listing streets starting with \"I\"\n",
    "main_page_url = r\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=I\"\n",
    "\n",
    "# Function to fetch the main page and extract all street links\n",
    "def fetch_main_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    street_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Streets.aspx?Name=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return street_links\n",
    "\n",
    "# Function to fetch the street page and extract all property links\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "# Function to fetch page and parse the HTML\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "# Function to process and extract data from the page\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "    \n",
    "    table_6 = tables[5]\n",
    "    table_9 = tables[8]\n",
    "    table_4 = tables[3]\n",
    "    table_2 = tables[1]\n",
    "    table_3 = tables[2]\n",
    "    table_14 = tables[13]\n",
    "    table_15 = tables[14]\n",
    "    table_7 = tables[6]\n",
    "    table_11 = tables[10]\n",
    "    table_12 = tables[11]\n",
    "    \n",
    "    owner = table_4.iloc[0, 1]\n",
    "    address = table_4.iloc[2, 1]\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    \n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15]:\n",
    "        table['Address'] = address\n",
    "        table['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 7 to a horizontal format\n",
    "    table_7_transposed = table_7.set_index(table_7.columns[0]).T.reset_index(drop=True)\n",
    "    table_7_transposed['Address'] = address\n",
    "    table_7_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 11 to a horizontal format\n",
    "    table_11_transposed = table_11.set_index(table_11.columns[0]).T.reset_index(drop=True)\n",
    "    table_11_transposed['Address'] = address\n",
    "    table_11_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 12 to a horizontal format\n",
    "    table_12_transposed = table_12.set_index(table_12.columns[0]).T.reset_index(drop=True)\n",
    "    table_12_transposed['Address'] = address\n",
    "    table_12_transposed['Pincode'] = pincode\n",
    "    \n",
    "    return table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_11_transposed, table_12_transposed\n",
    "\n",
    "# Initialize lists to store all scraped data\n",
    "combined_table_6 = []\n",
    "combined_table_9 = []\n",
    "combined_table_4_cleaned = []\n",
    "combined_table_2 = []\n",
    "combined_table_3 = []\n",
    "combined_table_14 = []\n",
    "combined_table_15 = []\n",
    "combined_table_7 = []\n",
    "combined_table_11 = []\n",
    "combined_table_12 = []\n",
    "\n",
    "# Fetch the list of street URLs from the main page\n",
    "street_urls = fetch_main_page(main_page_url)\n",
    "\n",
    "# Iterate through each street URL and scrape data\n",
    "for street_url in street_urls:\n",
    "    property_links = fetch_street_page(street_url)\n",
    "    \n",
    "    # Scrape and process each property URL\n",
    "    for url in property_links:\n",
    "        page_content = fetch_page(url)\n",
    "        table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_11_transposed, table_12_transposed = process_page(page_content)\n",
    "        \n",
    "        combined_table_6.append(table_6)\n",
    "        combined_table_9.append(table_9)\n",
    "        combined_table_4_cleaned.append(table_4_cleaned)\n",
    "        combined_table_2.append(table_2)\n",
    "        combined_table_3.append(table_3)\n",
    "        combined_table_14.append(table_14)\n",
    "        combined_table_15.append(table_15)\n",
    "        combined_table_7.append(table_7_transposed)\n",
    "        combined_table_11.append(table_11_transposed)\n",
    "        combined_table_12.append(table_12_transposed)\n",
    "\n",
    "# Concatenate all the data into final tables\n",
    "final_table_6 = pd.concat(combined_table_6, ignore_index=True)\n",
    "final_table_9 = pd.concat(combined_table_9, ignore_index=True)\n",
    "final_table_4_cleaned = pd.concat(combined_table_4_cleaned, ignore_index=True)\n",
    "final_table_2 = pd.concat(combined_table_2, ignore_index=True)\n",
    "final_table_3 = pd.concat(combined_table_3, ignore_index=True)\n",
    "final_table_14 = pd.concat(combined_table_14, ignore_index=True)\n",
    "final_table_15 = pd.concat(combined_table_15, ignore_index=True)\n",
    "final_table_7 = pd.concat(combined_table_7, ignore_index=True)\n",
    "final_table_11 = pd.concat(combined_table_11, ignore_index=True)\n",
    "final_table_12 = pd.concat(combined_table_12, ignore_index=True)\n",
    "\n",
    "# Create a new Excel file with 11 sheets (removed Table 8)\n",
    "with pd.ExcelWriter('property_details_combined_all_streets.xlsx') as writer:\n",
    "    final_table_6.to_excel(writer, sheet_name='Sales Data', index=False)\n",
    "    final_table_9.to_excel(writer, sheet_name='Area Details', index=False)\n",
    "    final_table_4_cleaned.to_excel(writer, sheet_name='Owner Details', index=False)\n",
    "    final_table_2.to_excel(writer, sheet_name='Appraisal', index=False)\n",
    "    final_table_3.to_excel(writer, sheet_name='Assessment', index=False)\n",
    "    final_table_14.to_excel(writer, sheet_name='Past Appraisal', index=False)\n",
    "    final_table_15.to_excel(writer, sheet_name='Past Assessment', index=False)\n",
    "    final_table_7.to_excel(writer, sheet_name='Building Details', index=False)\n",
    "    final_table_11.to_excel(writer, sheet_name='Use and Zone', index=False)\n",
    "    final_table_12.to_excel(writer, sheet_name='Size and Value', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b9b3c-47d7-4626-ba4a-e0bdd8daf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this scrip will scrap multiple street table 8 has removed becuz of concate issues\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# List of street URLs you want to scrape\n",
    "street_urls = [\n",
    "    r\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Name=OAKDALE%20ST\",\n",
    "    r\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Name=IMPERIAL%20ST\",\n",
    "    r\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Name=INDIAN%20AV\",\n",
    "    r\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Name=INDIAN%20AVE\"\n",
    "]\n",
    "\n",
    "# Function to fetch the street page and extract all property links\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "# Function to fetch page and parse the HTML\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "# Function to process and extract data from the page\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "    \n",
    "    table_6 = tables[5]\n",
    "    table_9 = tables[8]\n",
    "    table_4 = tables[3]\n",
    "    table_2 = tables[1]\n",
    "    table_3 = tables[2]\n",
    "    table_14 = tables[13]\n",
    "    table_15 = tables[14]\n",
    "    table_7 = tables[6]\n",
    "    table_11 = tables[10]\n",
    "    table_12 = tables[11]\n",
    "    \n",
    "    owner = table_4.iloc[0, 1]\n",
    "    address = table_4.iloc[2, 1]\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    \n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15]:\n",
    "        table['Address'] = address\n",
    "        table['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 7 to a horizontal format\n",
    "    table_7_transposed = table_7.set_index(table_7.columns[0]).T.reset_index(drop=True)\n",
    "    table_7_transposed['Address'] = address\n",
    "    table_7_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 11 to a horizontal format\n",
    "    table_11_transposed = table_11.set_index(table_11.columns[0]).T.reset_index(drop=True)\n",
    "    table_11_transposed['Address'] = address\n",
    "    table_11_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 12 to a horizontal format\n",
    "    table_12_transposed = table_12.set_index(table_12.columns[0]).T.reset_index(drop=True)\n",
    "    table_12_transposed['Address'] = address\n",
    "    table_12_transposed['Pincode'] = pincode\n",
    "    \n",
    "    return table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_11_transposed, table_12_transposed\n",
    "\n",
    "# Initialize lists to store all scraped data\n",
    "combined_table_6 = []\n",
    "combined_table_9 = []\n",
    "combined_table_4_cleaned = []\n",
    "combined_table_2 = []\n",
    "combined_table_3 = []\n",
    "combined_table_14 = []\n",
    "combined_table_15 = []\n",
    "combined_table_7 = []\n",
    "combined_table_11 = []\n",
    "combined_table_12 = []\n",
    "\n",
    "# Iterate through each street URL and scrape data\n",
    "for street_url in street_urls:\n",
    "    property_links = fetch_street_page(street_url)\n",
    "    \n",
    "    # Scrape and process each property URL\n",
    "    for url in property_links:\n",
    "        page_content = fetch_page(url)\n",
    "        table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_11_transposed, table_12_transposed = process_page(page_content)\n",
    "        \n",
    "        combined_table_6.append(table_6)\n",
    "        combined_table_9.append(table_9)\n",
    "        combined_table_4_cleaned.append(table_4_cleaned)\n",
    "        combined_table_2.append(table_2)\n",
    "        combined_table_3.append(table_3)\n",
    "        combined_table_14.append(table_14)\n",
    "        combined_table_15.append(table_15)\n",
    "        combined_table_7.append(table_7_transposed)\n",
    "        combined_table_11.append(table_11_transposed)\n",
    "        combined_table_12.append(table_12_transposed)\n",
    "\n",
    "# Concatenate all the data into final tables\n",
    "final_table_6 = pd.concat(combined_table_6, ignore_index=True)\n",
    "final_table_9 = pd.concat(combined_table_9, ignore_index=True)\n",
    "final_table_4_cleaned = pd.concat(combined_table_4_cleaned, ignore_index=True)\n",
    "final_table_2 = pd.concat(combined_table_2, ignore_index=True)\n",
    "final_table_3 = pd.concat(combined_table_3, ignore_index=True)\n",
    "final_table_14 = pd.concat(combined_table_14, ignore_index=True)\n",
    "final_table_15 = pd.concat(combined_table_15, ignore_index=True)\n",
    "final_table_7 = pd.concat(combined_table_7, ignore_index=True)\n",
    "final_table_11 = pd.concat(combined_table_11, ignore_index=True)\n",
    "final_table_12 = pd.concat(combined_table_12, ignore_index=True)\n",
    "\n",
    "# Create a new Excel file with 11 sheets (removed Table 8)\n",
    "with pd.ExcelWriter('property_details_combined_multiple_streets.xlsx') as writer:\n",
    "    final_table_6.to_excel(writer, sheet_name='Sales Data', index=False)\n",
    "    final_table_9.to_excel(writer, sheet_name='Area Details', index=False)\n",
    "    final_table_4_cleaned.to_excel(writer, sheet_name='Owner Details', index=False)\n",
    "    final_table_2.to_excel(writer, sheet_name='Appraisal', index=False)\n",
    "    final_table_3.to_excel(writer, sheet_name='Assessment', index=False)\n",
    "    final_table_14.to_excel(writer, sheet_name='Past Appraisal', index=False)\n",
    "    final_table_15.to_excel(writer, sheet_name='Past Assessment', index=False)\n",
    "    final_table_7.to_excel(writer, sheet_name='Building Details', index=False)\n",
    "    final_table_11.to_excel(writer, sheet_name='Use and Zone', index=False)\n",
    "    final_table_12.to_excel(writer, sheet_name='Size and Value', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f183b-443e-4133-ae63-f11832408d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL for the street\n",
    "street_url =r\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Name=OAKDALE%20ST\" #\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Name=PACIFIC%20ST\"\n",
    "\n",
    "# Function to fetch the street page and extract all property links\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all links to individual property details pages\n",
    "    property_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if 'Parcel.aspx?pid=' in href:  # Only interested in links to property pages\n",
    "            property_links.append('https://gis.vgsi.com/bridgeportct/' + href)\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "# Function to fetch page and parse the HTML\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the entire page content\n",
    "    page_content = soup.prettify()\n",
    "    \n",
    "    return page_content\n",
    "\n",
    "# Function to process and extract data from the page\n",
    "def process_page(page_content):\n",
    "    # Use pandas to read the tables from the HTML\n",
    "    tables = pd.read_html(page_content)\n",
    "\n",
    "    # Extract specific tables (Table 6, Table 9, and Table 4)\n",
    "    table_6 = tables[5]  # Table 6 is at index 5\n",
    "    table_9 = tables[8]  # Table 9 is at index 8\n",
    "    table_4 = tables[3]  # Table 4 is at index 3\n",
    "\n",
    "    # Extract \"Owner\" from the first row, second column in Table 4\n",
    "    owner = table_4.iloc[0, 1]\n",
    "\n",
    "    # Extract \"Address\" from the third row, second column in Table 4\n",
    "    address = table_4.iloc[2, 1]\n",
    "\n",
    "    # Extract the pincode from the address (assuming the pincode is the last part of the address)\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "\n",
    "    # Filter out \"Co-Owner\" and \"Address\" rows, keeping only \"Owner\"\n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "\n",
    "    # Add address and pincode columns to each table\n",
    "    table_6['Address'] = address\n",
    "    table_6['Pincode'] = pincode\n",
    "\n",
    "    table_9['Address'] = address\n",
    "    table_9['Pincode'] = pincode\n",
    "\n",
    "    # Using .loc to modify the cleaned table, avoiding the SettingWithCopyWarning\n",
    "    table_4_cleaned.loc[:, 'Address'] = address\n",
    "    table_4_cleaned.loc[:, 'Pincode'] = pincode\n",
    "\n",
    "    return table_6, table_9, table_4_cleaned\n",
    "\n",
    "# Fetch all the property links for the street\n",
    "property_links = fetch_street_page(street_url)\n",
    "print(f\"Found {len(property_links)} property links.\")\n",
    "\n",
    "# Lists to store the combined tables for all URLs\n",
    "combined_table_6 = []\n",
    "combined_table_9 = []\n",
    "combined_table_4_cleaned = []\n",
    "\n",
    "# Scrape and process each property URL\n",
    "for url in property_links:\n",
    "    print(f\"Scraping {url}...\")\n",
    "    page_content = fetch_page(url)\n",
    "    table_6, table_9, table_4_cleaned = process_page(page_content)\n",
    "    \n",
    "    # Append the processed tables to the combined lists\n",
    "    combined_table_6.append(table_6)\n",
    "    combined_table_9.append(table_9)\n",
    "    combined_table_4_cleaned.append(table_4_cleaned)\n",
    "\n",
    "# Concatenate the data from all URLs for each table\n",
    "final_table_6 = pd.concat(combined_table_6, ignore_index=True)\n",
    "final_table_9 = pd.concat(combined_table_9, ignore_index=True)\n",
    "final_table_4_cleaned = pd.concat(combined_table_4_cleaned, ignore_index=True)\n",
    "\n",
    "# Create a new Excel file with 3 sheets\n",
    "with pd.ExcelWriter('property_details_combined.xlsx') as writer:\n",
    "    final_table_6.to_excel(writer, sheet_name='Sales Data', index=False)\n",
    "    final_table_9.to_excel(writer, sheet_name='Area Details', index=False)\n",
    "    final_table_4_cleaned.to_excel(writer, sheet_name='Owner Details', index=False)\n",
    "\n",
    "print(\"All tables have been combined and saved to property_details_combined.xlsx.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18c112-7f82-40fb-97bd-9887ad1178ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=1763\",\n",
    "    \"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=2326\",\n",
    "    \"https://gis.vgsi.com/bridgeportct/Parcel.aspx?pid=1764\"\n",
    "]\n",
    "\n",
    "# Function to fetch page and parse the HTML\n",
    "def fetch_page(url):\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the entire page content\n",
    "    page_content = soup.prettify()\n",
    "    \n",
    "    return page_content\n",
    "\n",
    "# Function to process and extract data from the page\n",
    "def process_page(page_content):\n",
    "    # Use pandas to read the tables from the HTML\n",
    "    tables = pd.read_html(page_content)\n",
    "\n",
    "    # Extract specific tables (Table 6, Table 9, and Table 4)\n",
    "    table_6 = tables[5]  # Table 6 is at index 5\n",
    "    table_9 = tables[8]  # Table 9 is at index 8\n",
    "    table_4 = tables[3]  # Table 4 is at index 3\n",
    "\n",
    "    # Extract \"Owner\" from the first row, second column in Table 4\n",
    "    owner = table_4.iloc[0, 1]\n",
    "\n",
    "    # Extract \"Address\" from the third row, second column in Table 4\n",
    "    address = table_4.iloc[2, 1]\n",
    "\n",
    "    # Extract the pincode from the address (assuming the pincode is the last part of the address)\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "\n",
    "    # Filter out \"Co-Owner\" and \"Address\" rows, keeping only \"Owner\"\n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "\n",
    "    # Add address and pincode columns to each table\n",
    "    table_6['Address'] = address\n",
    "    table_6['Pincode'] = pincode\n",
    "\n",
    "    table_9['Address'] = address\n",
    "    table_9['Pincode'] = pincode\n",
    "\n",
    "    # Using .loc to modify the cleaned table, avoiding the SettingWithCopyWarning\n",
    "    table_4_cleaned.loc[:, 'Address'] = address\n",
    "    table_4_cleaned.loc[:, 'Pincode'] = pincode\n",
    "\n",
    "    return table_6, table_9, table_4_cleaned\n",
    "\n",
    "# List to store the tables from all URLs\n",
    "all_table_6 = []\n",
    "all_table_9 = []\n",
    "all_table_4_cleaned = []\n",
    "\n",
    "# Scrape and process each URL\n",
    "for url in urls:\n",
    "    print(f\"Scraping {url}...\")\n",
    "    page_content = fetch_page(url)\n",
    "    table_6, table_9, table_4_cleaned = process_page(page_content)\n",
    "    \n",
    "    # Append the processed tables to the list\n",
    "    all_table_6.append(table_6)\n",
    "    all_table_9.append(table_9)\n",
    "    all_table_4_cleaned.append(table_4_cleaned)\n",
    "\n",
    "# Create a new Excel file with each table in a separate sheet\n",
    "with pd.ExcelWriter('property_details_all_urls.xlsx') as writer:\n",
    "    # Write data from all URLs to the Excel file\n",
    "    for i, (table_6, table_9, table_4_cleaned) in enumerate(zip(all_table_6, all_table_9, all_table_4_cleaned)):\n",
    "        table_6.to_excel(writer, sheet_name=f'Sales Data {i+1}', index=False)\n",
    "        table_9.to_excel(writer, sheet_name=f'Area Details {i+1}', index=False)\n",
    "        table_4_cleaned.to_excel(writer, sheet_name=f'Owner Details {i+1}', index=False)\n",
    "\n",
    "print(\"All tables have been saved to property_details_all_urls.xlsx.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b8bc2-d795-48d3-a274-4d281fd27042",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# this scraps all the website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522d3cf-c8f3-4942-a49f-b629303af648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL for all streets\n",
    "base_url = \"https://gis.vgsi.com/bridgeportct/Streets.aspx\"\n",
    "\n",
    "# Function to fetch all letter-based street listing pages (A-Z)\n",
    "def fetch_all_letter_pages():\n",
    "    response = requests.get(base_url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    letter_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if \"Streets.aspx?Letter=\" in href:  \n",
    "            letter_links.append(\"https://gis.vgsi.com/bridgeportct/\" + href)\n",
    "    \n",
    "    return letter_links\n",
    "\n",
    "# Function to fetch all street links from a letter page\n",
    "def fetch_all_streets(letter_url):\n",
    "    response = requests.get(letter_url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    street_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if \"Streets.aspx?Name=\" in href:  \n",
    "            street_links.append(\"https://gis.vgsi.com/bridgeportct/\" + href)\n",
    "    \n",
    "    return street_links\n",
    "\n",
    "# Function to fetch all property links from a street page\n",
    "def fetch_street_page(street_url):\n",
    "    response = requests.get(street_url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    property_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if 'Parcel.aspx?pid=' in href:\n",
    "            property_links.append('https://gis.vgsi.com/bridgeportct/' + href)\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "# Function to fetch and parse the property page\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "# Function to extract property data\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "\n",
    "    table_6 = tables[5] if len(tables) > 5 else pd.DataFrame()\n",
    "    table_9 = tables[8] if len(tables) > 8 else pd.DataFrame()\n",
    "    table_4 = tables[3] if len(tables) > 3 else pd.DataFrame()\n",
    "\n",
    "    if table_4.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    owner = table_4.iloc[0, 1] if table_4.shape[0] > 0 else \"Unknown\"\n",
    "    address = table_4.iloc[2, 1] if table_4.shape[0] > 2 else \"Unknown\"\n",
    "\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "\n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    table_6['Address'], table_6['Pincode'] = address, pincode\n",
    "    table_9['Address'], table_9['Pincode'] = address, pincode\n",
    "    table_4_cleaned.loc[:, 'Address'], table_4_cleaned.loc[:, 'Pincode'] = address, pincode\n",
    "\n",
    "    return table_6, table_9, table_4_cleaned\n",
    "\n",
    "# Get all letter-based street listing pages\n",
    "letter_links = fetch_all_letter_pages()\n",
    "print(f\"Found {len(letter_links)} letter pages.\")\n",
    "\n",
    "# Lists to store final combined data\n",
    "final_table_6, final_table_9, final_table_4_cleaned = [], [], []\n",
    "\n",
    "# Iterate through each letter page\n",
    "for letter_url in letter_links:\n",
    "    print(f\"Processing letter page: {letter_url}\")\n",
    "    \n",
    "    # Get all street links for the current letter\n",
    "    street_links = fetch_all_streets(letter_url)\n",
    "    print(f\"Found {len(street_links)} streets.\")\n",
    "\n",
    "    # Iterate through each street\n",
    "    for street_url in street_links:\n",
    "        print(f\"Processing street: {street_url}\")\n",
    "        \n",
    "        # Get all property links in the street\n",
    "        property_links = fetch_street_page(street_url)\n",
    "        print(f\"Found {len(property_links)} properties.\")\n",
    "\n",
    "        # Process each property\n",
    "        for prop_url in property_links:\n",
    "            print(f\"Scraping property: {prop_url}\")\n",
    "            page_content = fetch_page(prop_url)\n",
    "            table_6, table_9, table_4_cleaned = process_page(page_content)\n",
    "\n",
    "            if table_6 is not None:\n",
    "                final_table_6.append(table_6)\n",
    "            if table_9 is not None:\n",
    "                final_table_9.append(table_9)\n",
    "            if table_4_cleaned is not None:\n",
    "                final_table_4_cleaned.append(table_4_cleaned)\n",
    "\n",
    "# Combine all data and save to an Excel file\n",
    "with pd.ExcelWriter('all_properties.xlsx') as writer:\n",
    "    if final_table_6:\n",
    "        pd.concat(final_table_6, ignore_index=True).to_excel(writer, sheet_name='Sales Data', index=False)\n",
    "    if final_table_9:\n",
    "        pd.concat(final_table_9, ignore_index=True).to_excel(writer, sheet_name='Area Details', index=False)\n",
    "    if final_table_4_cleaned:\n",
    "        pd.concat(final_table_4_cleaned, ignore_index=True).to_excel(writer, sheet_name='Owner Details', index=False)\n",
    "\n",
    "print(\"Scraping completed. Data saved to all_properties.xlsx.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15d959-5c51-43e8-9431-b8f5dec60d73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# IMP THIS SCRAPS ALL THE TABLE NEEDED FROM A STREET URL WE JUST HAVE TO MAKE IT FOR LETTER THATS IT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6ee1e-c95b-4d8c-969f-c38d6a67debb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL for the street\n",
    "street_url = r\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Name=OAKDALE%20ST\"\n",
    "\n",
    "# Function to fetch the street page and extract all property links\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "# Function to fetch page and parse the HTML\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "# Function to process and extract data from the page\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "    \n",
    "    table_6 = tables[5]\n",
    "    table_9 = tables[8]\n",
    "    table_4 = tables[3]\n",
    "    table_2 = tables[1]\n",
    "    table_3 = tables[2]\n",
    "    table_14 = tables[13]\n",
    "    table_15 = tables[14]\n",
    "    table_7 = tables[6]\n",
    "    table_8 = tables[7]\n",
    "    table_11 = tables[10]\n",
    "    table_12 = tables[11]\n",
    "    \n",
    "    owner = table_4.iloc[0, 1]\n",
    "    address = table_4.iloc[2, 1]\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    \n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15]:\n",
    "        table['Address'] = address\n",
    "        table['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 7 to a horizontal format\n",
    "    table_7_transposed = table_7.set_index(table_7.columns[0]).T.reset_index(drop=True)\n",
    "    table_7_transposed['Address'] = address\n",
    "    table_7_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 8 to a horizontal format\n",
    "    table_8_transposed = table_8.set_index(table_8.columns[0]).T.reset_index(drop=True)\n",
    "    table_8_transposed['Address'] = address\n",
    "    table_8_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 11 to a horizontal format\n",
    "    table_11_transposed = table_11.set_index(table_11.columns[0]).T.reset_index(drop=True)\n",
    "    table_11_transposed['Address'] = address\n",
    "    table_11_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 12 to a horizontal format\n",
    "    table_12_transposed = table_12.set_index(table_12.columns[0]).T.reset_index(drop=True)\n",
    "    table_12_transposed['Address'] = address\n",
    "    table_12_transposed['Pincode'] = pincode\n",
    "    \n",
    "    return table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_8_transposed, table_11_transposed, table_12_transposed\n",
    "\n",
    "# Fetch all property links\n",
    "property_links = fetch_street_page(street_url)\n",
    "\n",
    "combined_table_6 = []\n",
    "combined_table_9 = []\n",
    "combined_table_4_cleaned = []\n",
    "combined_table_2 = []\n",
    "combined_table_3 = []\n",
    "combined_table_14 = []\n",
    "combined_table_15 = []\n",
    "combined_table_7 = []\n",
    "combined_table_8 = []\n",
    "combined_table_11 = []\n",
    "combined_table_12 = []\n",
    "\n",
    "# Scrape and process each property URL\n",
    "for url in property_links:\n",
    "    page_content = fetch_page(url)\n",
    "    table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_8_transposed, table_11_transposed, table_12_transposed = process_page(page_content)\n",
    "    \n",
    "    combined_table_6.append(table_6)\n",
    "    combined_table_9.append(table_9)\n",
    "    combined_table_4_cleaned.append(table_4_cleaned)\n",
    "    combined_table_2.append(table_2)\n",
    "    combined_table_3.append(table_3)\n",
    "    combined_table_14.append(table_14)\n",
    "    combined_table_15.append(table_15)\n",
    "    combined_table_7.append(table_7_transposed)\n",
    "    combined_table_8.append(table_8_transposed)\n",
    "    combined_table_11.append(table_11_transposed)\n",
    "    combined_table_12.append(table_12_transposed)\n",
    "\n",
    "final_table_6 = pd.concat(combined_table_6, ignore_index=True)\n",
    "final_table_9 = pd.concat(combined_table_9, ignore_index=True)\n",
    "final_table_4_cleaned = pd.concat(combined_table_4_cleaned, ignore_index=True)\n",
    "final_table_2 = pd.concat(combined_table_2, ignore_index=True)\n",
    "final_table_3 = pd.concat(combined_table_3, ignore_index=True)\n",
    "final_table_14 = pd.concat(combined_table_14, ignore_index=True)\n",
    "final_table_15 = pd.concat(combined_table_15, ignore_index=True)\n",
    "final_table_7 = pd.concat(combined_table_7, ignore_index=True)\n",
    "final_table_8 = pd.concat(combined_table_8, ignore_index=True)\n",
    "final_table_11 = pd.concat(combined_table_11, ignore_index=True)\n",
    "final_table_12 = pd.concat(combined_table_12, ignore_index=True)\n",
    "\n",
    "# Create a new Excel file with 12 sheets\n",
    "with pd.ExcelWriter('property_details_combined.xlsx') as writer:\n",
    "    final_table_6.to_excel(writer, sheet_name='Sales Data', index=False)\n",
    "    final_table_9.to_excel(writer, sheet_name='Area Details', index=False)\n",
    "    final_table_4_cleaned.to_excel(writer, sheet_name='Owner Details', index=False)\n",
    "    final_table_2.to_excel(writer, sheet_name='Appraisal', index=False)\n",
    "    final_table_3.to_excel(writer, sheet_name='Assessment', index=False)\n",
    "    final_table_14.to_excel(writer, sheet_name='Past Appraisal', index=False)\n",
    "    final_table_15.to_excel(writer, sheet_name='Past Assessment', index=False)\n",
    "    final_table_7.to_excel(writer, sheet_name='Building Details', index=False)\n",
    "    final_table_8.to_excel(writer, sheet_name='Property Features', index=False)\n",
    "    final_table_11.to_excel(writer, sheet_name='Use and Zone', index=False)\n",
    "    final_table_12.to_excel(writer, sheet_name='Size and Value', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc32da-e236-4303-a68f-dbba41193de7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# final code with all tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86857ced-6602-432a-a9b5-a92d41e4e696",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# working on this to scrap the whole database of bridgeport\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Function to fetch the main street page and extract all the street URLs\n",
    "def fetch_main_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    street_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Streets.aspx?Name=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return street_links\n",
    "\n",
    "# Function to fetch the street page and extract all property links\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "# Function to fetch page and parse the HTML\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "# Function to process and extract data from the page\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "    \n",
    "    table_6 = tables[5]\n",
    "    table_9 = tables[8]\n",
    "    table_4 = tables[3]\n",
    "    table_2 = tables[1]\n",
    "    table_3 = tables[2]\n",
    "    table_14 = tables[13]\n",
    "    table_15 = tables[14]\n",
    "    table_7 = tables[6]\n",
    "    table_11 = tables[10]\n",
    "    table_12 = tables[11]\n",
    "    \n",
    "    owner = table_4.iloc[0, 1]\n",
    "    address = table_4.iloc[2, 1]\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    \n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15]:\n",
    "        table['Address'] = address\n",
    "        table['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 7 to a horizontal format\n",
    "    table_7_transposed = table_7.set_index(table_7.columns[0]).T.reset_index(drop=True)\n",
    "    table_7_transposed['Address'] = address\n",
    "    table_7_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 11 to a horizontal format\n",
    "    table_11_transposed = table_11.set_index(table_11.columns[0]).T.reset_index(drop=True)\n",
    "    table_11_transposed['Address'] = address\n",
    "    table_11_transposed['Pincode'] = pincode\n",
    "    \n",
    "    # Transform Table 12 to a horizontal format\n",
    "    table_12_transposed = table_12.set_index(table_12.columns[0]).T.reset_index(drop=True)\n",
    "    table_12_transposed['Address'] = address\n",
    "    table_12_transposed['Pincode'] = pincode\n",
    "    \n",
    "    return table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_11_transposed, table_12_transposed\n",
    "\n",
    "# Initialize lists to store all scraped data\n",
    "combined_table_6 = []\n",
    "combined_table_9 = []\n",
    "combined_table_4_cleaned = []\n",
    "combined_table_2 = []\n",
    "combined_table_3 = []\n",
    "combined_table_14 = []\n",
    "combined_table_15 = []\n",
    "combined_table_7 = []\n",
    "combined_table_11 = []\n",
    "combined_table_12 = []\n",
    "\n",
    "# Main URL for scraping all street links\n",
    "main_url = \"https://gis.vgsi.com/bridgeportct/Streets.aspx\"\n",
    "street_links = fetch_main_page(main_url)\n",
    "\n",
    "# Iterate through each street URL and scrape data\n",
    "for street_url in street_links:\n",
    "    print(f\"Processing {street_url}\")\n",
    "    \n",
    "    # Fetch all property links from the street page\n",
    "    property_links = fetch_street_page(street_url)\n",
    "    \n",
    "    # Scrape and process each property URL\n",
    "    for url in property_links:\n",
    "        page_content = fetch_page(url)\n",
    "        table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_11_transposed, table_12_transposed = process_page(page_content)\n",
    "        \n",
    "        combined_table_6.append(table_6)\n",
    "        combined_table_9.append(table_9)\n",
    "        combined_table_4_cleaned.append(table_4_cleaned)\n",
    "        combined_table_2.append(table_2)\n",
    "        combined_table_3.append(table_3)\n",
    "        combined_table_14.append(table_14)\n",
    "        combined_table_15.append(table_15)\n",
    "        combined_table_7.append(table_7_transposed)\n",
    "        combined_table_11.append(table_11_transposed)\n",
    "        combined_table_12.append(table_12_transposed)\n",
    "\n",
    "# Concatenate all the data into final tables\n",
    "final_table_6 = pd.concat(combined_table_6, ignore_index=True)\n",
    "final_table_9 = pd.concat(combined_table_9, ignore_index=True)\n",
    "final_table_4_cleaned = pd.concat(combined_table_4_cleaned, ignore_index=True)\n",
    "final_table_2 = pd.concat(combined_table_2, ignore_index=True)\n",
    "final_table_3 = pd.concat(combined_table_3, ignore_index=True)\n",
    "final_table_14 = pd.concat(combined_table_14, ignore_index=True)\n",
    "final_table_15 = pd.concat(combined_table_15, ignore_index=True)\n",
    "final_table_7 = pd.concat(combined_table_7, ignore_index=True)\n",
    "final_table_11 = pd.concat(combined_table_11, ignore_index=True)\n",
    "final_table_12 = pd.concat(combined_table_12, ignore_index=True)\n",
    "\n",
    "# Create a new Excel file with 11 sheets (removed Table 8)\n",
    "with pd.ExcelWriter('property_details_combined_all_streets_full_website.xlsx') as writer:\n",
    "    final_table_6.to_excel(writer, sheet_name='Sales Data', index=False)\n",
    "    final_table_9.to_excel(writer, sheet_name='Area Details', index=False)\n",
    "    final_table_4_cleaned.to_excel(writer, sheet_name='Owner Details', index=False)\n",
    "    final_table_2.to_excel(writer, sheet_name='Appraisal', index=False)\n",
    "    final_table_3.to_excel(writer, sheet_name='Assessment', index=False)\n",
    "    final_table_14.to_excel(writer, sheet_name='Past Appraisal', index=False)\n",
    "    final_table_15.to_excel(writer, sheet_name='Past Assessment', index=False)\n",
    "    final_table_7.to_excel(writer, sheet_name='Building Details', index=False)\n",
    "    final_table_11.to_excel(writer, sheet_name='Use and Zone', index=False)\n",
    "    final_table_12.to_excel(writer, sheet_name='Size and Value', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97962152-e553-485c-9ff0-8fbad893f865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def fetch_main_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    street_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Streets.aspx?Name=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return street_links\n",
    "\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "    \n",
    "    table_6 = tables[5] table_9 = tables[8] table_4 = tables[3]table_2 = tables[1]table_3 = tables[2]\n",
    "    table_14 = tables[13]\n",
    "    table_15 = tables[14]\n",
    "    table_7 = tables[6]\n",
    "    table_8 = tables[7] \n",
    "    table_11 = tables[10]\n",
    "    table_12 = tables[11]\n",
    "    \n",
    "    owner = table_4.iloc[0, 1]\n",
    "    address = table_4.iloc[2, 1]\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    \n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_8]:\n",
    "        table['Address'] = address\n",
    "        table['Pincode'] = pincode\n",
    "    \n",
    "    table_7_transposed = table_7.set_index(table_7.columns[0]).T.reset_index(drop=True)\n",
    "    table_7_transposed['Address'] = address\n",
    "    table_7_transposed['Pincode'] = pincode\n",
    "    \n",
    "    table_11_transposed = table_11.set_index(table_11.columns[0]).T.reset_index(drop=True)\n",
    "    table_11_transposed['Address'] = address\n",
    "    table_11_transposed['Pincode'] = pincode\n",
    "    \n",
    "    table_12_transposed = table_12.set_index(table_12.columns[0]).T.reset_index(drop=True)\n",
    "    table_12_transposed['Address'] = address\n",
    "    table_12_transposed['Pincode'] = pincode\n",
    "    \n",
    "    return table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_8, table_11_transposed, table_12_transposed\n",
    "\n",
    "combined_data = {i: [] for i in range(11)}\n",
    "urls = [\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=I\", \"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=K\"]\n",
    "\n",
    "for main_url in urls:\n",
    "    street_links = fetch_main_page(main_url)\n",
    "    \n",
    "    for street_url in street_links:\n",
    "        print(f\"Processing {street_url}\")\n",
    "        property_links = fetch_street_page(street_url)\n",
    "        \n",
    "        for url in property_links:\n",
    "            try:\n",
    "                page_content = fetch_page(url)\n",
    "                extracted_data = process_page(page_content)\n",
    "                \n",
    "                for idx, table in enumerate(extracted_data):\n",
    "                    combined_data[idx].append(table)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "final_dataframes = {idx: pd.concat(tables, ignore_index=True) for idx, tables in combined_data.items() if tables}\n",
    "\n",
    "with pd.ExcelWriter('property_details_I_K.xlsx') as writer:\n",
    "    sheet_names = ['Sales Data', 'Area Details', 'Owner Details', 'Appraisal', 'Assessment', 'Past Appraisal', 'Past Assessment', 'Building Details', 'Table 8', 'Use and Zone', 'Size and Value']\n",
    "    \n",
    "    for idx, df in final_dataframes.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_names[idx], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c2ed4-6729-4b7e-8a11-a53fe45ee314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def fetch_main_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    street_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Streets.aspx?Name=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return street_links\n",
    "\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "    \n",
    "    table_6 = tables[5]\n",
    "    table_9 = tables[8]\n",
    "    table_4 = tables[3]\n",
    "    table_2 = tables[1]\n",
    "    table_3 = tables[2]\n",
    "    table_14 = tables[13]\n",
    "    table_15 = tables[14]\n",
    "    table_7 = tables[6]\n",
    "    table_8 = tables[7] \n",
    "    table_11 = tables[10]\n",
    "    table_12 = tables[11]\n",
    "    \n",
    "    owner = table_4.iloc[0, 1]\n",
    "    address = table_4.iloc[2, 1]\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    \n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_8]:\n",
    "        table['Address'] = address\n",
    "        table['Pincode'] = pincode\n",
    "    \n",
    "    table_7_transposed = table_7.set_index(table_7.columns[0]).T.reset_index(drop=True)\n",
    "    table_7_transposed['Address'] = address\n",
    "    table_7_transposed['Pincode'] = pincode\n",
    "    \n",
    "    table_11_transposed = table_11.set_index(table_11.columns[0]).T.reset_index(drop=True)\n",
    "    table_11_transposed['Address'] = address\n",
    "    table_11_transposed['Pincode'] = pincode\n",
    "    \n",
    "    table_12_transposed = table_12.set_index(table_12.columns[0]).T.reset_index(drop=True)\n",
    "    table_12_transposed['Address'] = address\n",
    "    table_12_transposed['Pincode'] = pincode\n",
    "    \n",
    "    return table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_8, table_11_transposed, table_12_transposed\n",
    "\n",
    "combined_data = {i: [] for i in range(11)}\n",
    "urls = [\n",
    "    \"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=A\",  # New URL for Q Street\n",
    "    \"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=B\"   # New URL for U Street\n",
    "]\n",
    "\n",
    "# Scrape data for Q and U streets\n",
    "for main_url in urls:\n",
    "    street_links = fetch_main_page(main_url)\n",
    "    \n",
    "    for street_url in street_links:\n",
    "        print(f\"Processing {street_url}\")\n",
    "        property_links = fetch_street_page(street_url)\n",
    "        \n",
    "        for url in property_links:\n",
    "            try:\n",
    "                page_content = fetch_page(url)\n",
    "                extracted_data = process_page(page_content)\n",
    "                \n",
    "                for idx, table in enumerate(extracted_data):\n",
    "                    combined_data[idx].append(table)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "final_dataframes = {idx: pd.concat(tables, ignore_index=True) for idx, tables in combined_data.items() if tables}\n",
    "\n",
    "# Read the existing Excel file and append the new data\n",
    "with pd.ExcelWriter('property_details_I_K.xlsx', mode='a', if_sheet_exists='overlay') as writer:  # Open in append mode\n",
    "    sheet_names = ['Sales Data', 'Area Details', 'Owner Details', 'Appraisal', 'Assessment', 'Past Appraisal', 'Past Assessment', 'Building Details', 'Table 8', 'Use and Zone', 'Size and Value']\n",
    "    \n",
    "    for idx, df in final_dataframes.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_names[idx], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350036a-55de-4d28-a6f5-86fff27ff68c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad512d-f341-473f-90f3-b3a824609fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9374950-5185-40fd-afab-2fd5a6fe1aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def fetch_main_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    street_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Streets.aspx?Name=' in link['href'] ]\n",
    "    return street_links\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href'] ]\n",
    "    return property_links\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content) table_6 = tables[5] table_9 = tables[8] table_4 = tables[3]table_2 = tables[1]table_3 = tables[2] table_14 = tables[13]\n",
    "    table_15 = tables[14] table_7 = tables[6] table_8 = tables[7] table_11 = tables[10] table_12 = tables[11]\n",
    "    address = table_4.iloc[2, 1]\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_8]:\n",
    "        table['Address'] = address\n",
    "        table['Pincode'] = pincode\n",
    "    table_7_transposed = table_7.set_index(table_7.columns[0]).T.reset_index(drop=True)\n",
    "    table_7_transposed['Address'] = address\n",
    "    table_7_transposed['Pincode'] = pincode\n",
    "    table_11_transposed = table_11.set_index(table_11.columns[0]).T.reset_index(drop=True)\n",
    "    table_11_transposed['Address'] = address\n",
    "    table_11_transposed['Pincode'] = pincode\n",
    "    table_12_transposed = table_12.set_index(table_12.columns[0]).T.reset_index(drop=True)\n",
    "    table_12_transposed['Address'] = address\n",
    "    table_12_transposed['Pincode'] = pincode\n",
    "    return table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_7_transposed, table_8, table_11_transposed, table_12_transposed\n",
    "def process_property(url):\n",
    "    try:\n",
    "        page_content = fetch_page(url)\n",
    "        return process_page(page_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return None\n",
    "combined_data = {i: [] for i in range(11)}\n",
    "urls = [ \"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=I\",\"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=J\"]\n",
    "for main_url in urls:\n",
    "    street_links = fetch_main_page(main_url)\n",
    "    for street_url in street_links:\n",
    "    property_links = fetch_street_page(street_url)\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            results = list(executor.map(process_property, property_links))\n",
    "             for extracted_data in results:\n",
    "            if extracted_data:\n",
    "                for idx, table in enumerate(extracted_data):\n",
    "                    combined_data[idx].append(table)\n",
    "final_dataframes = {idx: pd.concat(tables, ignore_index=True) for idx, tables in combined_data.items() if tables}\n",
    "with pd.ExcelWriter('hproperty_details.xlsx', mode='w') as writer:\n",
    "    sheet_names = ['Sales Data', 'Area Details', 'Owner Details', 'Appraisal', 'Assessment', 'Past Appraisal', 'Past Assessment', 'Building Details', 'Table 8', 'Use and Zone', 'Size and Value']\n",
    "     for idx, df in final_dataframes.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_names[idx], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc6ea7-fc80-422e-9ac6-955e05756708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def fetch_main_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    street_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Streets.aspx?Name=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return street_links\n",
    "\n",
    "def fetch_street_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_links = [\n",
    "        'https://gis.vgsi.com/bridgeportct/' + link['href']\n",
    "        for link in soup.find_all('a', href=True) if 'Parcel.aspx?pid=' in link['href']\n",
    "    ]\n",
    "    \n",
    "    return property_links\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'html.parser').prettify()\n",
    "\n",
    "def process_page(page_content):\n",
    "    tables = pd.read_html(page_content)\n",
    "    \n",
    "    table_6 = tables[5]\n",
    "    table_9 = tables[8]\n",
    "    table_4 = tables[3]\n",
    "    table_2 = tables[1]\n",
    "    table_3 = tables[2]\n",
    "    table_14 = tables[13]\n",
    "    table_15 = tables[14]\n",
    "    table_7 = tables[6]\n",
    "    table_8 = tables[7] \n",
    "    table_11 = tables[10]\n",
    "    table_12 = tables[11]\n",
    "    \n",
    "    address = table_4.iloc[2, 1] if len(table_4) > 2 else 'Unknown'\n",
    "    pincode_match = re.search(r'\\d{5}(-\\d{4})?', address)\n",
    "    pincode = pincode_match.group(0) if pincode_match else 'Unknown'\n",
    "    \n",
    "    table_4_cleaned = table_4[table_4[0] == 'Owner']\n",
    "    \n",
    "    for table in [table_6, table_9, table_4_cleaned, table_2, table_3, table_14, table_15, table_8]:\n",
    "        if not table.empty:\n",
    "            table.loc[:, 'Address'] = address\n",
    "            table.loc[:, 'Pincode'] = pincode\n",
    "    \n",
    "    def transpose_table(table):\n",
    "        if not table.empty:\n",
    "            table_transposed = table.set_index(table.columns[0]).T.reset_index(drop=True)\n",
    "            table_transposed.loc[:, 'Address'] = address\n",
    "            table_transposed.loc[:, 'Pincode'] = pincode\n",
    "            return table_transposed\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return (\n",
    "        table_6, table_9, table_4_cleaned, table_2, table_3, table_14, \n",
    "        table_15, transpose_table(table_7), table_8, transpose_table(table_11), transpose_table(table_12)\n",
    "    )\n",
    "\n",
    "def process_property(url):\n",
    "    try:\n",
    "        page_content = fetch_page(url)\n",
    "        return process_page(page_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "combined_data = {i: [] for i in range(11)}\n",
    "urls = [\n",
    "    \"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=T\",\n",
    "    \"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=U\",\n",
    "    \"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=V\",\n",
    "    \"https://gis.vgsi.com/bridgeportct/Streets.aspx?Letter=Y\"\n",
    "]\n",
    "\n",
    "for main_url in urls:\n",
    "    street_links = fetch_main_page(main_url)\n",
    "    \n",
    "    for street_url in street_links:\n",
    "        property_links = fetch_street_page(street_url)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "            results = list(executor.map(process_property, property_links))\n",
    "            \n",
    "        for extracted_data in results:\n",
    "            if extracted_data:\n",
    "                for idx, table in enumerate(extracted_data):\n",
    "                    if table is not None and not table.empty:\n",
    "                        combined_data[idx].append(table.reset_index(drop=True))\n",
    "\n",
    "final_dataframes = {idx: pd.concat(tables, ignore_index=True) for idx, tables in combined_data.items() if tables}\n",
    "\n",
    "with pd.ExcelWriter('TUVYproperty_details.xlsx', mode='w') as writer:\n",
    "    sheet_names = ['Sales Data', 'Area Details', 'Owner Details', 'Appraisal', 'Assessment', 'Past Appraisal', 'Past Assessment', 'Building Details', 'Table 8', 'Use and Zone', 'Size and Value']\n",
    "    \n",
    "    for idx, df in final_dataframes.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_names[idx], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc734ae-94fe-4f1e-bb0b-3fe5c1c7580d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c250d25-22ae-4c66-b53a-190d0b9eaa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}